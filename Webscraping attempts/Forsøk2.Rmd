---
title: "Webscraping2"
output: pdf_document
---

```{r setup}
library(tidyverse)
library(rvest)
library(xml2)
```

## Kode fra dag 7
Her laster vi ned de første linkene til hovedkategoriene på forsiden til SSB. Det er 23 linker vi laster ned. 
  
```{r}
ssb <- download_html(
  url = "https://www.ssb.no/",
  file = "./Data/SSB.html"
)

# hovedkategorier <- ["/arbeid-og-lonn"]

hovedkategorier <- read_html("./Data/SSB.html") %>%
  html_nodes('a[class="ssb-link with-icon"]') %>% # klassenavnet for 'hovedkategoriene', vi leser ut hver node som oppfyller dette kravet
  html_attr("href") %>% #forteller at det er en link vi vil ha
  str_c("https://ssb.no", .) #setter sammen hva vi henter med html_nodes med dette
```

Laster ned alle nettsidene:

Iterere gjennom alle 23 html-filene lasted ned lokalt for å finne underlinkene:

```{r}
# Step 2: Download all the webpages from the links you just made
hovedkategori_navn <- str_remove(hovedkategorier, "https://ssb.no") 

for(i in 1:length(hovedkategorier)) {
  download.file(hovedkategorier[[i]], 
                destfile = str_c("./Data/hovedkategorier/", hovedkategori_navn[i], ".html")) 
  # Download one html-file after another into the folder hovedkategorier
    Sys.sleep(2) 
  # Setting a timer of two seconds each time we download a webpage.  
}
```

##underkategorier
Lager en liste for hver hovedkategori med alle underkategori html pathene, nå kan vi ha en løkke som går gjennom listen i listen for å laste ned sidene 
```{r}
for (i in 1:length(hovedkategori_navn)) { 
# For each i in every element from place number one to the last place in hovedkategorier_lastet (given by length(hovedkategorier_lastet)) 
  filnavn = paste("Data/hovedkategorier", hovedkategori_navn[i], ".html", sep="")
  # Read the html-page for each i 
    tmp <- read_html(filnavn)   %>% # create a tmp list with the undercategories
    html_nodes('a[class="ssb-category-link"]') %>% #again with the nodes
    html_attr("href") %>% 
    str_c("https://ssb.no", .)
  # insert list of undercategory in list: structure
   underkategorier[[i]] <- tmp
}
```

Laster ned hver underkategori som html, i egen mappe i data mappen  (fungerer ikke atm)
```{r}
    liste = underkategorier[[1]]
# We download the underkategori html files
    string_som_skal_fjernes <- paste("https://ssb.no", hovedkategori_navn[1], sep="")
    string_som_skal_fjernes
  underkategori_navn <- str_remove(liste, string_som_skal_fjernes) 
  underkategori_navn
```
#laster ned html filene for alle underkategoriene. Det er i disse html filene statistikken ligger. Vi må nå inn i hver statistikk, og i hver statistikk for vi kjørt program som samler inn 'om-statistikken' delen. 

```{r}
getwd()
for(i in 1:length(underkategorier)) { #23 elementer
    liste = underkategorier[[i]]
# We download the underkategori html files
    string_som_skal_fjernes <- paste("https://ssb.no", hovedkategori_navn[i], sep="")
  underkategori_navn <- str_remove(liste, string_som_skal_fjernes) 
  for(j in 1:length(liste)){
    download.file(liste[j],
                  destfile = str_c("./Data/underkategorier/", underkategori_navn[j], ".html")) 
    # Download one html-file after another into the folder hovedkategorier
      Sys.sleep(2) 
    # Setting a timer of two seconds each time we download a webpage. 
  }
}
```



## foreløpig irrelevant
```{r}
# Step 3: Loop over every link and extract the text using the same procedure as shown before
info <- list()  
# Make a list-object where you can put the output from the loop 
for (i in 1:length(links)) { 
# For each i in every element from place number one to the last place in links (given by length(links)) 
   
  page <- read_html(links[[i]])  
  # Read the html-page for each i 
   
  page <- page %>% # Use this page 
    html_elements("p") %>% # And fetch the paragraph elements 
    html_text() # Then extract the text from these elements 
   
  info[[i]] <- page  
  # Place the text for each link into its respective place in the info-object
   
}
```

